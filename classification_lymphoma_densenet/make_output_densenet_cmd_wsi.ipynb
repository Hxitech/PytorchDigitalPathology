{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T14:27:13.600689Z",
     "start_time": "2020-04-07T14:27:13.584753Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import sklearn.feature_extraction.image\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,'/mnt/data/home/pjl54/WSI_handling')\n",
    "import wsi\n",
    "\n",
    "from torchvision.models import DenseNet\n",
    "\n",
    "#-----helper function to split data into batches\n",
    "def divide_batch(l, n): \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n]\n",
    "\n",
    "# ----- parse command line arguments\n",
    "\n",
    "input_pattern=['/mnt/ccipd_data/UH_Bladder_Cancer_Project/Blad170830/Blad_2.tif','/mnt/ccipd_data/UH_Bladder_Cancer_Project/Blad170830/Blad_3.tif']\n",
    "resolution = 0.5\n",
    "model = '/mnt/data/home/pjl54/bladder/densenet/bladder_densenet_best_model.pth'\n",
    "color=None\n",
    "annotation='largest'\n",
    "patchsize=256\n",
    "batchsize=10\n",
    "outdir='./output/'\n",
    "gpuid=0\n",
    "force=False\n",
    "basepath=''\n",
    "desired_mask_mpp=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNet\n",
    "\n",
    "te_model = '/mnt/data/home/pjl54/bladder/bladderTE_1mpp_256p.pth'\n",
    "te_mpp = 1;\n",
    "te_device = 2\n",
    "checkpoint = torch.load(te_model, map_location=lambda storage, loc: storage) #load checkpoint to CPU and then put to device https://discuss.pytorch.org/t/saving-and-loading-torch-models-on-2-machines-with-different-number-of-gpu-devices/6666\n",
    "model = UNet(n_classes=checkpoint[\"n_classes\"], in_channels=checkpoint[\"in_channels\"],\n",
    "             padding=checkpoint[\"padding\"], depth=checkpoint[\"depth\"], wf=checkpoint[\"wf\"],\n",
    "             up_mode=checkpoint[\"up_mode\"], batch_norm=checkpoint[\"batch_norm\"]).to(te_device)\n",
    "model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T14:27:14.439252Z",
     "start_time": "2020-04-07T14:27:14.351205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: \t415554\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUTPUT_DIR = outdir\n",
    "\n",
    "batch_size = batchsize\n",
    "patch_size = patchsize\n",
    "base_stride_size = patch_size//2\n",
    "\n",
    "\n",
    "# ----- load network\n",
    "device = torch.device(gpuid if gpuid!=-2 and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "checkpoint = torch.load(model, map_location=lambda storage, loc: storage) #load checkpoint to CPU and then put to device https://discuss.pytorch.org/t/saving-and-loading-torch-models-on-2-machines-with-different-number-of-gpu-devices/6666\n",
    "\n",
    "model = DenseNet(growth_rate=checkpoint[\"growth_rate\"], block_config=checkpoint[\"block_config\"],\n",
    "                 num_init_features=checkpoint[\"num_init_features\"], bn_size=checkpoint[\"bn_size\"],\n",
    "                 drop_rate=checkpoint[\"drop_rate\"], num_classes=checkpoint[\"num_classes\"]).to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "model.eval()\n",
    "\n",
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T14:27:15.349765Z",
     "start_time": "2020-04-07T14:27:15.338771Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----- get file list\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "files = []\n",
    "basepath = basepath  #\n",
    "basepath = basepath + os.sep if len(\n",
    "    basepath) > 0 else \"\"  # if the user supplied a different basepath, make sure it ends with an os.sep\n",
    "\n",
    "if len(input_pattern) > 1:  # bash has sent us a list of files\n",
    "    files = input_pattern\n",
    "elif input_pattern[0].endswith(\"tsv\"):  # user sent us an input file\n",
    "    # load first column here and store into files\n",
    "    with open(input_pattern[0], 'r') as f:\n",
    "        for line in f:\n",
    "            if line[0] == \"#\":\n",
    "                continue\n",
    "            files.append(basepath + line.strip().split(\"\\t\")[0])\n",
    "else:  # user sent us a wildcard, need to use glob to find files\n",
    "    files = glob.glob(basepath + input_pattern[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T15:00:37.818389Z",
     "start_time": "2020-04-07T14:27:35.466487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on file: \t /mnt/ccipd_data/UH_Bladder_Cancer_Project/Blad170830/Blad_2.tif\n",
      "class/count: \t0\t1156\n",
      "class/count: \t1\t59360\n",
      "predicted class:\t1\n",
      "working on file: \t /mnt/ccipd_data/UH_Bladder_Cancer_Project/Blad170830/Blad_3.tif\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-c4a9505eefc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_points\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbatch_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_points\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;31m#             print(np.shape(arr_out))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#             arr_out = arr_out.reshape(-1,patch_size,patch_size,3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-c4a9505eefc8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_points\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbatch_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_points\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;31m#             print(np.shape(arr_out))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#             arr_out = arr_out.reshape(-1,patch_size,patch_size,3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/home/pjl54/WSI_handling/wsi.py\u001b[0m in \u001b[0;36mget_tile\u001b[0;34m(self, desired_mpp, coords, wh, wh_at_base)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coords_scn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscn_wh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaled_wh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/home/pjl54/WSI_handling/wsi.py\u001b[0m in \u001b[0;36mread_region\u001b[0;34m(self, coords, target_layer, wh)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;34m\"\"\"Returns an RGB image of the desired region, will use more libraries when implemented, for now just Openslide\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"osh\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# openslide returns an alpha channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/openslide/__init__.py\u001b[0m in \u001b[0;36mread_region\u001b[0;34m(self, location, level, size)\u001b[0m\n\u001b[1;32m    221\u001b[0m         function is not premultiplied.\"\"\"\n\u001b[1;32m    222\u001b[0m         return lowlevel.read_region(self._osr, location[0], location[1],\n\u001b[0;32m--> 223\u001b[0;31m                 level, size[0], size[1])\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/openslide/lowlevel.py\u001b[0m in \u001b[0;36mread_region\u001b[0;34m(slide, x, y, level, w, h)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGBA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc_uint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0m_read_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_load_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/openslide/lowlevel.py\u001b[0m in \u001b[0;36m_check_error\u001b[0;34m(result, func, args)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;31m# check if the library got into an error state after each library call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_check_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------ work on files\n",
    "for fname in files:\n",
    "\n",
    "    fname = fname.strip()\n",
    "    newfname_class = \"%s/%s_class.png\" % (OUTPUT_DIR, os.path.basename(fname)[0:-4])\n",
    "\n",
    "    print(f\"working on file: \\t {fname}\")\n",
    "\n",
    "    # if not force and os.path.exists(newfname_class):\n",
    "    #     print(\"Skipping as output file exists\")\n",
    "    #     continue\n",
    "    #\n",
    "    # cv2.imwrite(newfname_class, np.zeros(shape=(1, 1)))\n",
    "\n",
    "    output_fname = outdir + fname + '_grade_results'\n",
    "    xml_dir = fname[0:fname.rfind(os.path.sep)]\n",
    "    xml_fname = xml_dir + os.path.sep + os.path.basename(fname)[0:os.path.basename(fname).rfind('.')] + '.xml'\n",
    "\n",
    "    img = wsi.wsi(fname,xml_fname)\n",
    "    \n",
    "    stride_size = int(base_stride_size * (resolution/img[\"mpp\"]))\n",
    "    \n",
    "#     if(annotation.lower() == 'wsi'):\n",
    "#         img_dims = [0,0,w[\"img_dims\"][0],w[\"img_dims\"][1]]\n",
    "#     else:\n",
    "#     img_dims = img.get_dimensions_of_annotation(color,annotation)\n",
    "    stride_size_converted = img.get_coord_at_mpp(stride_size,input_mpp=img[\"mpps\"][0],output_mpp=desired_mask_mpp)\n",
    "    [mask_small, resize_factor] = img.mask_out_annotation(desired_mpp=desired_mask_mpp,colors_to_use=color)            \n",
    "    \n",
    "    mask_small = mask_small[list(range(0,np.shape(mask_small)[0],stride_size_converted)),:]            \n",
    "    mask_small = mask_small[:,list(range(0,np.shape(mask_small)[1],stride_size_converted))]            \n",
    "\n",
    "    [rs,cs]=(mask_small>0).nonzero()\n",
    "    rs = [r*stride_size_converted for r in rs]\n",
    "    cs = [c*stride_size_converted for c in cs]\n",
    "\n",
    "    rs = [img.get_coord_at_mpp(r,img[\"mpps\"][0],desired_mask_mpp) for r in rs]\n",
    "    cs = [img.get_coord_at_mpp(c,img[\"mpps\"][0],desired_mask_mpp) for c in cs]\n",
    "\n",
    "    goods = np.ones(np.shape(rs)[0])\n",
    "    for k in range(0,np.shape(rs)[0]):\n",
    "\n",
    "        te_tile = wsi_img.get_tile(coords=(cs[k],rs[k]),wh=(int(patch_size*(te_mpp/model_mpp)),int(patch_size*(te_mpp/model_mpp))),desired_mpp=te_mpp)\n",
    "        if((np.sum(te_tile[:,:,1]>220)/np.size(te_tile[:,:,1]))>0.30):\n",
    "            goods[k] = False\n",
    "        else:    \n",
    "            arr_out_gpu = torch.from_numpy(np.expand_dims(te_tile,axis=0).transpose(0,3,1,2) / 255).type('torch.FloatTensor').to(te_device)\n",
    "            output_batch = model(arr_out_gpu)\n",
    "            output = output_batch[0,:,:,:].detach().cpu().numpy()\n",
    "            te_map = output.argmax(axis=0)==1\n",
    "\n",
    "            if((np.sum(te_map)/np.size(te_map))<0.50):                \n",
    "                goods[k] = False\n",
    "\n",
    "\n",
    "    cs = [c for idx,c in enumerate(cs) if goods[idx]]\n",
    "    rs = [r for idx,r in enumerate(rs) if goods[idx]]\n",
    "    \n",
    "    grid_points = [(x,y) for x in cs for y in rs]\n",
    "\n",
    "    points_split = divide_batch(grid_points,batch_size)\n",
    "\n",
    "    #in case we have a large network, lets cut the list of tiles into batches\n",
    "    output = np.zeros((0,checkpoint[\"num_classes\"]))\n",
    "    for i,batch_points in enumerate(points_split):\n",
    "\n",
    "        batch_arr = np.array([img.get_tile(resolution,coords,(patch_size,patch_size)) for coords in batch_points])\n",
    "#             print(np.shape(arr_out))\n",
    "#             arr_out = arr_out.reshape(-1,patch_size,patch_size,3)\n",
    "\n",
    "        arr_out_gpu = torch.from_numpy(batch_arr.transpose(0, 3, 1, 2) / 255).type('torch.FloatTensor').to(device)\n",
    "\n",
    "        # ---- get results\n",
    "        output_batch = model(arr_out_gpu)\n",
    "\n",
    "        # --- pull from GPU and append to rest of output \n",
    "        output_batch = output_batch.detach().cpu().numpy()\n",
    "\n",
    "        output = np.append(output,output_batch,axis=0)\n",
    "\n",
    "\n",
    "    tileclass = np.argmax(output, axis=1)\n",
    "    predc,predccounts=np.unique(tileclass, return_counts=True)\n",
    "    for c,cc in zip(predc,predccounts):\n",
    "        print(f\"class/count: \\t{c}\\t{cc}\")\n",
    "\n",
    "    print(f\"predicted class:\\t{predc[np.argmax(predccounts)]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
