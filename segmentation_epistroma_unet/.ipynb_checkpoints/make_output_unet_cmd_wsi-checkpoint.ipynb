{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2\n",
    "7/11/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import sklearn.feature_extraction.image\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1,'/mnt/data/home/pjl54/WSI_handling')\n",
    "import wsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#-----helper function to split data into batches\n",
    "def divide_batch(l, n): \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- parse command line arguments\n",
    "parser = argparse.ArgumentParser(description='Make output for entire image using Unet')\n",
    "parser.add_argument('input_pattern',\n",
    "                    help=\"input filename pattern. try: *.png, or tsv file containing list of files to analyze\",\n",
    "                    nargs=\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument('-r', '--resolution', help=\"image resolution in microns per pixel\", default=1, type=float)\n",
    "parser.add_argument('-c', '--color', help=\"annotation color to use, default None\", default='green', type=str)\n",
    "parser.add_argument('-a', '--annotation', help=\"annotation index to use, default largest\", default='wsi', type=str)\n",
    "\n",
    "parser.add_argument('-p', '--patchsize', help=\"patchsize, default 256\", default=256, type=int)\n",
    "parser.add_argument('-s', '--batchsize', help=\"batchsize for controlling GPU memory usage, default 10\", default=10, type=int)\n",
    "parser.add_argument('-o', '--outdir', help=\"outputdir, default ./output/\", default=\"./output/\", type=str)\n",
    "parser.add_argument('-m', '--model', help=\"model\", default=\"best_model.pth\", type=str)\n",
    "parser.add_argument('-i', '--gpuid', help=\"id of gpu to use\", default=0, type=int)\n",
    "parser.add_argument('-f', '--force', help=\"force regeneration of output even if it exists\", default=False,\n",
    "                    action=\"store_true\")\n",
    "parser.add_argument('-b', '--basepath',\n",
    "                    help=\"base path to add to file names, helps when producing data using tsv file as input\",\n",
    "                    default=\"\", type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (args.input_pattern):\n",
    "    parser.error('No images selected with input pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = args.outdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "batch_size = args.batchsize\n",
    "patch_size = args.patchsize\n",
    "base_stride_size = patch_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- load network\n",
    "device = torch.device(args.gpuid if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(args.model, map_location=lambda storage, loc: storage) #load checkpoint to CPU and then put to device https://discuss.pytorch.org/t/saving-and-loading-torch-models-on-2-machines-with-different-number-of-gpu-devices/6666\n",
    "model = UNet(n_classes=checkpoint[\"n_classes\"], in_channels=checkpoint[\"in_channels\"],\n",
    "             padding=checkpoint[\"padding\"], depth=checkpoint[\"depth\"], wf=checkpoint[\"wf\"],\n",
    "             up_mode=checkpoint[\"up_mode\"], batch_norm=checkpoint[\"batch_norm\"]).to(device)\n",
    "model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- get file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "basepath = args.basepath  #\n",
    "basepath = basepath + os.sep if len(\n",
    "    basepath) > 0 else \"\"  # if the user supplied a different basepath, make sure it ends with an os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if len(args.input_pattern) > 1:  # bash has sent us a list of files\n",
    "    files = args.input_pattern\n",
    "elif args.input_pattern[0].endswith(\"tsv\"):  # user sent us an input file\n",
    "    # load first column here and store into files\n",
    "    with open(args.input_pattern[0], 'r') as f:\n",
    "        for line in f:\n",
    "            if line[0] == \"#\":\n",
    "                continue\n",
    "            files.append(basepath + line.strip().split(\"\\t\")[0])\n",
    "else:  # user sent us a wildcard, need to use glob to find files\n",
    "    files = glob.glob(args.basepath + args.input_pattern[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ work on files\n",
    "for fname in files:\n",
    "    \n",
    "    fname = fname.strip()\n",
    "    newfname_class = \"%s/%s_class.png\" % (OUTPUT_DIR, os.path.basename(fname)[0:fname.rfind('.')])\n",
    "\n",
    "    print(f\"working on file: \\t {fname}\")\n",
    "    print(f\"saving to : \\t {newfname_class}\")\n",
    "\n",
    "    if not args.force and os.path.exists(newfname_class):\n",
    "        print(\"Skipping as output file exists\")\n",
    "        continue\n",
    "        \n",
    "    cv2.imwrite(newfname_class, np.zeros(shape=(1, 1)))                                            \n",
    "    \n",
    "    xml_dir = fname[0:fname.rfind(os.path.sep)]\n",
    "    xml_fname = xml_dir + os.path.sep + os.path.basename(fname)[0:os.path.basename(fname).rfind('.')] + '.xml'\n",
    "\n",
    "    img = wsi.wsi(fname,xml_fname)\n",
    "    \n",
    "    stride_size = int(base_stride_size * (args.resolution/img[\"mpp\"]))\n",
    "    \n",
    "    if(args.annotation.lower() == 'wsi'):\n",
    "        img_dims = [0,0,w[\"img_dims\"][0],w[\"img_dims\"][1]]\n",
    "    else:\n",
    "        img_dims = img.get_dimensions_of_annotation(args.color,args.annotation)\n",
    "    \n",
    "    if img_dims:\n",
    "    \n",
    "        x_start = int(img_dims[0])\n",
    "        y_start = int(img_dims[1])\n",
    "        w_orig = int(img_dims[2])\n",
    "        h_orig = int(img_dims[3])\n",
    "\n",
    "\n",
    "        w = int(w_orig + (patch_size - (w_orig % patch_size)))\n",
    "        h = int(h_orig + (patch_size - (h_orig % patch_size)))\n",
    "\n",
    "        x_points = range(x_start-stride_size//2,x_start+w+stride_size//2,stride_size)\n",
    "        y_points = range(y_start-stride_size//2,y_start+h+stride_size//2,stride_size)\n",
    "\n",
    "        grid_points = [(x,y) for x in x_points for y in y_points]\n",
    "        points_split = divide_batch(grid_points,batch_size)\n",
    "\n",
    "        #in case we have a large network, lets cut the list of tiles into batches\n",
    "        output = np.zeros((len(grid_points),checkpoint[\"n_classes\"],patch_size,patch_size))\n",
    "        for i,batch_points in enumerate(points_split):\n",
    "\n",
    "            batch_arr = np.array([img.get_tile(args.resolution,coords,(patch_size,patch_size)) for coords in batch_points])\n",
    "#             print(np.shape(arr_out))\n",
    "#             arr_out = arr_out.reshape(-1,patch_size,patch_size,3)\n",
    "\n",
    "            arr_out_gpu = torch.from_numpy(batch_arr.transpose(0, 3, 1, 2) / 255).type('torch.FloatTensor').to(device)\n",
    "\n",
    "            # ---- get results\n",
    "            output_batch = model(arr_out_gpu)\n",
    "\n",
    "            # --- pull from GPU and append to rest of output \n",
    "            output_batch = output_batch.detach().cpu().numpy()\n",
    "\n",
    "            output[((i+1)*batch_size - batch_size):((i+1)*batch_size),:,:,:] = output_batch\n",
    "#             output = np.append(output,output_batch,axis=0)\n",
    "\n",
    "\n",
    "        output = output.transpose((0, 3, 2, 1))\n",
    "\n",
    "        #turn from a single list into a matrix of tiles\n",
    "        output = output.reshape(len(x_points),len(y_points),patch_size,patch_size,output.shape[3])\n",
    "\n",
    "        #remove the padding from each tile, we only keep the center\n",
    "        output=output[:,:,base_stride_size//2:-base_stride_size//2,base_stride_size//2:-base_stride_size//2,:]\n",
    "\n",
    "        #turn all the tiles into an image\n",
    "        output=np.concatenate(np.concatenate(output,1),1)\n",
    "\n",
    "        #incase there was extra padding to get a multiple of patch size, remove that as well\n",
    "        mask = img.get_annotated_region(args.resolution,args.color,args.annotation,return_img=False)\n",
    "        mask = mask[1]\n",
    "\n",
    "        output = output.transpose((1,0,2))\n",
    "        output = output[0:mask.shape[0], 0:mask.shape[1], :] #remove paddind, crop back\n",
    "        output = output.argmax(axis=2) * (256 / (output.shape[-1] - 1) - 1)\n",
    "        output = cv2.bitwise_and(output,output,mask=np.uint8(mask))\n",
    "        output = np.uint8((output>0) + (mask==0))*255\n",
    "        \n",
    "        # --- save output\n",
    "\n",
    "        # cv2.imwrite(newfname_class, (output.argmax(axis=2) * (256 / (output.shape[-1] - 1) - 1)).astype(np.uint8))\n",
    "        cv2.imwrite(newfname_class, output)\n",
    "        \n",
    "    else:\n",
    "        print('No annotation of color')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
