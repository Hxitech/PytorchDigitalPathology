{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2\n",
    "7/11/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:55.683977Z",
     "start_time": "2020-10-16T21:59:54.551283Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "from WSI_handling import wsi\n",
    "from unet import UNet\n",
    "\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:55.691649Z",
     "start_time": "2020-10-16T21:59:55.687134Z"
    },
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#-----helper function to split data into batches\n",
    "def divide_batch(l, n): \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:55.714500Z",
     "start_time": "2020-10-16T21:59:55.694394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=[], dest='input_pattern', nargs='*', const=None, default=None, type=None, choices=None, help='input filename pattern. try: *.png, or tsv file containing list of files to analyze', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- parse command line arguments\n",
    "parser = argparse.ArgumentParser(description='Make output for entire image using Unet')\n",
    "parser.add_argument('input_pattern',\n",
    "                    help=\"input filename pattern. try: *.png, or tsv file containing list of files to analyze\",\n",
    "                    nargs=\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:55.729069Z",
     "start_time": "2020-10-16T21:59:55.716930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-b', '--basepath'], dest='basepath', nargs=None, const=None, default='', type=<class 'str'>, choices=None, help='base path to add to file names, helps when producing data using tsv file as input', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('-r', '--resolution', help=\"image resolution in microns per pixel\", default=1, type=float)\n",
    "parser.add_argument('-c', '--color', help=\"annotation color to use, default None\", default='green', type=str)\n",
    "parser.add_argument('-a', '--annotation', help=\"annotation index to use, default largest\", default='wsi', type=str)\n",
    "\n",
    "parser.add_argument('-p', '--patchsize', help=\"patchsize, default 256\", default=256, type=int)\n",
    "parser.add_argument('-s', '--batchsize', help=\"batchsize for controlling GPU memory usage, default 10\", default=10, type=int)\n",
    "parser.add_argument('-o', '--outdir', help=\"outputdir, default ./output/\", default=\"./output/\", type=str)\n",
    "parser.add_argument('-m', '--model', help=\"model\", default=\"best_model.pth\", type=str)\n",
    "parser.add_argument('-i', '--gpuid', help=\"id of gpu to use\", default=0, type=int)\n",
    "parser.add_argument('-f', '--force', help=\"force regeneration of output even if it exists\", default=False,\n",
    "                    action=\"store_true\")\n",
    "parser.add_argument('-b', '--basepath',\n",
    "                    help=\"base path to add to file names, helps when producing data using tsv file as input\",\n",
    "                    default=\"\", type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:55.739866Z",
     "start_time": "2020-10-16T21:59:55.730988Z"
    }
   },
   "outputs": [],
   "source": [
    "#args = parser.parse_args(['-s150','-o/mnt/rstor/CSE_BME_AXM788/home/pjl54/test','-r0.25','-a','largest','-m','/home/pjl54/models/nucer_unet_best_model.pth','/mnt/rstor/CSE_BME_AXM788/data/TCGA_Bladder_Cancer/Diagnostic_Images/Elloitt/1stAnnotatedDrElloitt/TCGA-4Z-AA7O-01Z-00-DX1.svs'])\n",
    "args = parser.parse_args(['-s30','-i0','-p512','-o/mnt/data/home/pjl54/test','-r1','-aall','-m','/mnt/data/home/pjl54/models/crib_PL_1mpp_512p.pth','/mnt/ccipd_data/TCGA_PRAD/2018Jan14/TCGA-EJ-5494-01Z-00-DX1.svs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:55.752543Z",
     "start_time": "2020-10-16T21:59:55.742044Z"
    }
   },
   "outputs": [],
   "source": [
    "if not (args.input_pattern):\n",
    "    parser.error('No images selected with input pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:55.767763Z",
     "start_time": "2020-10-16T21:59:55.754352Z"
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = args.outdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:55.779165Z",
     "start_time": "2020-10-16T21:59:55.770952Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "batch_size = args.batchsize\n",
    "patch_size = args.patchsize\n",
    "base_stride_size = patch_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:55.849173Z",
     "start_time": "2020-10-16T21:59:55.781194Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----- load network\n",
    "device = torch.device(args.gpuid if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:59.949031Z",
     "start_time": "2020-10-16T21:59:55.851471Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (down_path): ModuleList(\n",
       "    (0): UNetConvBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): UNetConvBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): UNetConvBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): UNetConvBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): UNetConvBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU()\n",
       "        (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_path): ModuleList(\n",
       "    (0): UNetUpBlock(\n",
       "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block): UNetConvBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): UNetUpBlock(\n",
       "      (up): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block): UNetConvBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): UNetUpBlock(\n",
       "      (up): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block): UNetConvBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): UNetUpBlock(\n",
       "      (up): ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv_block): UNetConvBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): ReLU()\n",
       "          (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (last): Conv2d(8, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(args.model, map_location=lambda storage, loc: storage) #load checkpoint to CPU and then put to device https://discuss.pytorch.org/t/saving-and-loading-torch-models-on-2-machines-with-different-number-of-gpu-devices/6666\n",
    "model = UNet(n_classes=checkpoint[\"n_classes\"], in_channels=checkpoint[\"in_channels\"],\n",
    "             padding=checkpoint[\"padding\"], depth=checkpoint[\"depth\"], wf=checkpoint[\"wf\"],\n",
    "             up_mode=checkpoint[\"up_mode\"], batch_norm=checkpoint[\"batch_norm\"]).to(device)\n",
    "model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:59.959745Z",
     "start_time": "2020-10-16T21:59:59.951443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: \t487298\n"
     ]
    }
   ],
   "source": [
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- get file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:59.970774Z",
     "start_time": "2020-10-16T21:59:59.962198Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T21:59:59.982396Z",
     "start_time": "2020-10-16T21:59:59.973111Z"
    }
   },
   "outputs": [],
   "source": [
    "files = []\n",
    "basepath = args.basepath  #\n",
    "basepath = basepath + os.sep if len(\n",
    "    basepath) > 0 else \"\"  # if the user supplied a different basepath, make sure it ends with an os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T22:00:00.003034Z",
     "start_time": "2020-10-16T21:59:59.984638Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if len(args.input_pattern) > 1:  # bash has sent us a list of files\n",
    "    files = args.input_pattern\n",
    "elif args.input_pattern[0].endswith(\"tsv\"):  # user sent us an input file\n",
    "    # load first column here and store into files\n",
    "    with open(args.input_pattern[0], 'r') as f:\n",
    "        for line in f:\n",
    "            if line[0] == \"#\":\n",
    "                continue\n",
    "            files.append(basepath + line.strip().split(\"\\t\")[0])\n",
    "else:  # user sent us a wildcard, need to use glob to find files\n",
    "    files = glob.glob(args.basepath + args.input_pattern[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T22:00:00.030000Z",
     "start_time": "2020-10-16T22:00:00.005625Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(img_dims,patch_size,stride_size,base_stride_size,batch_size,args,img,annotation):\n",
    "    x_start = int(img_dims[0])\n",
    "    y_start = int(img_dims[1])\n",
    "    w_orig = img.get_coord_at_mpp(img_dims[2] - x_start,input_mpp=img['mpp'],output_mpp=args.resolution)\n",
    "    h_orig = img.get_coord_at_mpp(img_dims[3] - y_start,input_mpp=img['mpp'],output_mpp=args.resolution)\n",
    "\n",
    "    w = int(w_orig + (patch_size - (w_orig % patch_size)))\n",
    "    h = int(h_orig + (patch_size - (h_orig % patch_size)))\n",
    "\n",
    "    base_edge_length = base_stride_size*int(math.sqrt(batch_size))        \n",
    "    \n",
    "    # need to make sure we don't end up with a last row/column smaller than patch_size\n",
    "    h = h + patch_size if base_edge_length - (h % base_edge_length) else h\n",
    "    w = w + patch_size if base_edge_length - (w % base_edge_length) else w\n",
    "\n",
    "    roi = img.get_tile(args.resolution,(x_start-stride_size//2,y_start-stride_size//2),(w+base_stride_size,h+base_stride_size))\n",
    "    x_points = range(0,np.shape(roi)[0],base_stride_size*int(math.sqrt(batch_size)))\n",
    "    y_points = range(0,np.shape(roi)[1],base_stride_size*int(math.sqrt(batch_size)))\n",
    "    grid_points = [(x,y) for x in x_points for y in y_points]                \n",
    "\n",
    "    output = np.zeros([np.shape(roi)[0],np.shape(roi)[1]],dtype='uint8')\n",
    "\n",
    "    for i,batch_points in enumerate(grid_points):\n",
    "\n",
    "        # get the tile of the batch\n",
    "        big_patch = roi[batch_points[0]:(batch_points[0]+base_edge_length+base_stride_size),batch_points[1]:(batch_points[1]+base_edge_length+base_stride_size),:]\n",
    "\n",
    "        big_patch_gpu = torch.from_numpy(big_patch).type('torch.FloatTensor').to(device)\n",
    "        # split the tile into patch_size patches\n",
    "        batch_arr = torch.stack(([big_patch_gpu[x:x+patch_size,y:y+patch_size,:] for y in range(0,np.shape(big_patch_gpu)[1]-base_stride_size,base_stride_size) for x in range(0,np.shape(big_patch_gpu)[0]-base_stride_size,base_stride_size)]))        \n",
    "        batch_arr = batch_arr.permute(0,3,1,2) / 255\n",
    "\n",
    "        # ---- get results\n",
    "        output_batch = model(batch_arr)\n",
    "        output_batch = output_batch.argmax(axis=1)\n",
    "\n",
    "        #remove the padding from each tile, we only keep the center            \n",
    "        output_batch = output_batch[:,base_stride_size//2:-base_stride_size//2,base_stride_size//2:-base_stride_size//2]            \n",
    "\n",
    "        # --- pull from GPU and append to rest of output \n",
    "        output_batch = output_batch.detach().cpu().numpy()            \n",
    "\n",
    "        reconst = np.concatenate(np.concatenate(output_batch.reshape(int(np.shape(big_patch)[1]/(patch_size//2))-1,int(np.shape(big_patch)[0]/(patch_size//2))-1,base_stride_size,base_stride_size),axis=2),axis=0)\n",
    "\n",
    "        output[batch_points[0]:(batch_points[0]+np.shape(big_patch)[0]-base_stride_size),batch_points[1]:(batch_points[1]+np.shape(big_patch)[1]-base_stride_size)] = reconst\n",
    "\n",
    "\n",
    "    if(args.annotation.lower() != 'wsi'):\n",
    "    #in case there was extra padding to get a multiple of patch size, remove that as well\n",
    "        _,mask = img.get_annotated_region(args.resolution,args.color,annotation,return_img=False)            \n",
    "        output = output[0:mask.shape[0], 0:mask.shape[1]] #remove paddind, crop back\n",
    "        output = np.bitwise_and(output>0,mask>0)*255\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T22:23:11.102016Z",
     "start_time": "2020-10-16T22:00:00.032093Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on file: \t /mnt/ccipd_data/TCGA_PRAD/2018Jan14/TCGA-EJ-5494-01Z-00-DX1.svs\n",
      "Working on annotation 0\n",
      "Elapsed time = 364.05009365081787\n",
      "Working on annotation 1\n",
      "Elapsed time = 1026.6292686462402\n"
     ]
    }
   ],
   "source": [
    "for fname in files:    \n",
    "    fname = fname.strip()\n",
    "    \n",
    "    if(args.annotation.lower() != 'all'):        \n",
    "    \n",
    "        newfname_class = \"%s/%s_class.png\" % (OUTPUT_DIR, Path(fname).stem)\n",
    "\n",
    "        if not args.force and os.path.exists(newfname_class):\n",
    "            print(\"Skipping as output file exists\")\n",
    "            continue\n",
    "        print(f\"working on file: \\t {fname}\")\n",
    "        print(f\"saving to : \\t {newfname_class}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        cv2.imwrite(newfname_class, np.zeros(shape=(1, 1)))                                            \n",
    "\n",
    "    xml_fname = Path(fname).with_suffix('.xml')\n",
    "    if not os.path.exists(xml_fname):\n",
    "        xml_fname = Path(fname).with_suffix('.json')\n",
    "\n",
    "    if os.path.exists(xml_fname):\n",
    "        img = wsi(fname,xml_fname)\n",
    "        stride_size = int(base_stride_size * (args.resolution/img[\"mpp\"]))\n",
    "\n",
    "        if(args.annotation.lower() == 'all'):        \n",
    "            annotations_todo = len(img.get_points(args.color,[]))\n",
    "            print(f\"working on file: \\t {fname}\")            \n",
    "\n",
    "            for k in range(0,annotations_todo):                \n",
    "                print('Working on annotation ' + str(k))\n",
    "                start_time = time.time()\n",
    "                img_dims = img.get_dimensions_of_annotation(args.color,k)\n",
    "\n",
    "                newfname_class = \"%s/%s_%d_class.png\" % (OUTPUT_DIR, Path(fname).stem,k)\n",
    "\n",
    "                if args.force or not os.path.exists(newfname_class):\n",
    "                    output = run_model(img_dims,patch_size,stride_size,base_stride_size,batch_size,args,img,annotation=k)        \n",
    "                    cv2.imwrite(newfname_class, output)                \n",
    "\n",
    "                output = None\n",
    "                print('Elapsed time = ' + str(time.time()-start_time))\n",
    "\n",
    "        else:            \n",
    "\n",
    "            if(args.annotation.lower() == 'wsi'):\n",
    "                img_dims = [0,0,img[\"img_dims\"][0][0],img[\"img_dims\"][0][1]]\n",
    "            else:\n",
    "                img_dims = img.get_dimensions_of_annotation(args.color,args.annotation)\n",
    "\n",
    "            if img_dims:\n",
    "                output = run_model(img_dims,patch_size,stride_size,base_stride_size,batch_size,args,img,annotation=args.annotation)        \n",
    "                cv2.imwrite(newfname_class, output)\n",
    "                output = None\n",
    "                print('Elapsed time = ' + str(time.time()-start_time))\n",
    "\n",
    "            else:\n",
    "                print('No annotation of color')\n",
    "    else:\n",
    "        print('Could not find ' + str(xml_fname))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
