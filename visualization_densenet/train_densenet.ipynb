{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname=\"synthetic\"\n",
    "gpuid=0\n",
    "\n",
    "# --- densenet params\n",
    "#these parameters get fed directly into the densenet class, and more description of them can be discovered there\n",
    "n_classes= 2    #number of classes in the data mask that we'll aim to predict\n",
    "in_channels= 3  #input channel of the data, RGB = 3\n",
    "\n",
    "\n",
    "growth_rate=8 \n",
    "block_config=(4, 4, 4, 4)\n",
    "num_init_features=2\n",
    "bn_size=4\n",
    "drop_rate=0\n",
    "\n",
    "\n",
    "\n",
    "# --- training params\n",
    "batch_size=64\n",
    "patch_size=224 #currently, this needs to be 224 due to densenet architecture\n",
    "num_epochs = 100\n",
    "phases = [\"train\",\"val\"] #how many phases did we create databases for?\n",
    "validation_phases= [\"val\"] #when should we do valiation? note that validation is *very* time consuming, so as opposed to doing for both training and validation, we do it only for vlaidation at the end of the epoch\n",
    "                           #additionally, using simply [], will skip validation entirely, drastically speeding things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import tables\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import DenseNet\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for pretty printing of current time and remaining time\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent+.00001)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='Quadro M2200', major=5, minor=2, total_memory=4096MB, multi_processor_count=8)\n"
     ]
    }
   ],
   "source": [
    "#specify if we should use a GPU (cuda) or only the CPU\n",
    "print(torch.cuda.get_device_properties(gpuid))\n",
    "torch.cuda.set_device(gpuid)\n",
    "device = torch.device(f'cuda:{gpuid}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\torchvision\\models\\densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: \t58212\n"
     ]
    }
   ],
   "source": [
    "#build the model according to the paramters specified above and copy it to the GPU. finally print out the number of trainable parameters\n",
    " \n",
    "model = DenseNet(growth_rate=growth_rate, block_config=block_config,\n",
    "                 num_init_features=num_init_features, bn_size=bn_size, drop_rate=drop_rate, num_classes=n_classes).to(device)\n",
    "\n",
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this defines our dataset class which will be used by the dataloader\n",
    "class Dataset(object):\n",
    "    def __init__(self, fname ,img_transform=None):\n",
    "        #nothing special here, just internalizing the constructor parameters\n",
    "        self.fname=fname\n",
    "\n",
    "        self.img_transform=img_transform\n",
    "        \n",
    "        with tables.open_file(self.fname,'r') as db:\n",
    "            self.classsizes=db.root.classsizes[:]\n",
    "            self.nitems=db.root.imgs.shape[0]\n",
    "        \n",
    "        self.imgs = None\n",
    "        self.labels = None\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #opening should be done in __init__ but seems to be\n",
    "        #an issue with multithreading so doing here. need to do it everytime, otherwise hdf5 crashes\n",
    "\n",
    "        with tables.open_file(self.fname,'r') as db:\n",
    "            self.imgs=db.root.imgs\n",
    "            self.labels=db.root.labels\n",
    "\n",
    "            #get the requested image\n",
    "            img = self.imgs[index,::]\n",
    "            img = img[:,:,None].repeat(3,axis=2) #convert to 3 channel RGB\n",
    "            label = self.labels[index] \n",
    "        \n",
    "        img_new = img\n",
    "        \n",
    "        if self.img_transform is not None:\n",
    "            img_new = self.img_transform(img)\n",
    "\n",
    "        return img_new, label, img\n",
    "    def __len__(self):\n",
    "        return self.nitems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size:\t10000\n",
      "val dataset size:\t100\n"
     ]
    }
   ],
   "source": [
    "img_transform = transforms.Compose([\n",
    "     transforms.ToPILImage(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(size=(patch_size,patch_size),pad_if_needed=True), #these need to be in a reproducible order, first affine transforms and then color\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "dataset={}\n",
    "dataLoader={}\n",
    "for phase in phases: #now for each of the phases, we're creating the dataloader\n",
    "                     #interestingly, given the batch size, i've not seen any improvements from using a num_workers>0\n",
    "    \n",
    "    dataset[phase]=Dataset(f\"./{dataname}_{phase}.pytable\", img_transform=img_transform)\n",
    "    dataLoader[phase]=DataLoader(dataset[phase], batch_size=256, \n",
    "                                shuffle=True, num_workers=0,pin_memory=True) \n",
    "    print(f\"{phase} dataset size:\\t{len(dataset[phase])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize a single example to verify that it is correct\n",
    "(img, label, img_old)=dataset[\"train\"][24]\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))  # 1 row, 2 columns\n",
    "\n",
    "#build output showing patch after augmentation and original patch\n",
    "ax[0].imshow(np.moveaxis(img.numpy(),0,-1))\n",
    "ax[1].imshow(img_old)\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters()) #adam is going to be the most robust, though perhaps not the best performing, typically a good place to start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4907, 0.5093], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#we have the ability to weight individual classes, in this case we'll do so based on their presense in the trainingset\n",
    "#to avoid biasing any particular class\n",
    "nclasses = dataset[\"train\"].classsizes.shape[0]\n",
    "class_weight=dataset[\"train\"].classsizes\n",
    "class_weight = torch.from_numpy(1-class_weight/class_weight.sum()).type('torch.FloatTensor').to(device)\n",
    "\n",
    "print(class_weight) #show final used weights, make sure that they're reasonable before continouing\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = class_weight) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "#%lprun\n",
    "# %%prun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainnetwork():\n",
    "    writer=SummaryWriter() #open the tensorboard visualiser\n",
    "    best_loss_on_test = np.Infinity\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        #zero out epoch based performance variables \n",
    "        all_acc = {key: 0 for key in phases} \n",
    "        all_loss = {key: torch.zeros(0).to(device) for key in phases} #keep this on GPU for greatly improved performance\n",
    "        cmatrix = {key: np.zeros((n_classes,n_classes)) for key in phases}\n",
    "\n",
    "        for phase in phases: #iterate through both training and validation states\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else: #when in eval mode, we don't want parameters to be updated\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            for ii , (X, label, img_orig) in enumerate(dataLoader[phase]): #for each of the batches\n",
    "                X = X.to(device)  # [Nbatch, 3, H, W]\n",
    "                label = label.type('torch.LongTensor').to(device)  # [Nbatch, 1] with class indices (0, 1, 2,...n_classes)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'): #dynamically set gradient computation, in case of validation, this isn't needed\n",
    "                                                                #disabling is good practice and improves inference time\n",
    "\n",
    "                    prediction = model(X)  # [Nbatch, Nclass]\n",
    "                    loss = criterion(prediction, label)\n",
    "\n",
    "\n",
    "                    if phase==\"train\": #in case we're in train mode, need to do back propogation\n",
    "                        optim.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optim.step()\n",
    "                        train_loss = loss\n",
    "\n",
    "\n",
    "                    all_loss[phase]=torch.cat((all_loss[phase],loss.detach().view(1,-1)))\n",
    "\n",
    "                    if phase in validation_phases: #if this phase is part of validation, compute confusion matrix\n",
    "                        p=prediction.detach().cpu().numpy()\n",
    "                        cpredflat=np.argmax(p,axis=1).flatten()\n",
    "                        yflat=label.cpu().numpy().flatten()\n",
    "\n",
    "                        cmatrix[phase]=cmatrix[phase]+confusion_matrix(yflat,cpredflat, labels=range(nclasses))\n",
    "\n",
    "            all_acc[phase]=(cmatrix[phase]/cmatrix[phase].sum()).trace()\n",
    "            all_loss[phase] = all_loss[phase].cpu().numpy().mean()\n",
    "\n",
    "            #save metrics to tensorboard\n",
    "            writer.add_scalar(f'{phase}/loss', all_loss[phase], epoch)\n",
    "            if phase in validation_phases:\n",
    "                writer.add_scalar(f'{phase}/acc', all_acc[phase], epoch)\n",
    "                for r in range(nclasses):\n",
    "                    for c in range(nclasses): #essentially write out confusion matrix\n",
    "                        writer.add_scalar(f'{phase}/{r}{c}', cmatrix[phase][r][c],epoch)\n",
    "\n",
    "        print('%s ([%d/%d] %d%%), train loss: %.4f test loss: %.4f' % (timeSince(start_time, (epoch+1) / num_epochs), \n",
    "                                                     epoch+1, num_epochs ,(epoch+1) / num_epochs * 100, all_loss[\"train\"], all_loss[\"val\"]),end=\"\")    \n",
    "\n",
    "        #if current loss is the best we've seen, save model state with all variables\n",
    "        #necessary for recreation\n",
    "        if all_loss[\"val\"] < best_loss_on_test:\n",
    "            best_loss_on_test = all_loss[\"val\"]\n",
    "            print(\"  **\")\n",
    "            state = {'epoch': epoch + 1,\n",
    "             'model_dict': model.state_dict(),\n",
    "             'optim_dict': optim.state_dict(),\n",
    "             'best_loss_on_test': all_loss,\n",
    "             'n_classes': n_classes,\n",
    "             'in_channels': in_channels,\n",
    "             'growth_rate':growth_rate,\n",
    "             'block_config':block_config,\n",
    "             'num_init_features':num_init_features,\n",
    "             'bn_size':bn_size,\n",
    "             'drop_rate':drop_rate}\n",
    "\n",
    "\n",
    "            torch.save(state, f\"{dataname}_densenet_best_model.pth\")\n",
    "        else:\n",
    "            print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 9s (- 115m 1s) ([1/100] 1%), train loss: 0.5159 test loss: 0.5040  **\n",
      "2m 22s (- 116m 30s) ([2/100] 2%), train loss: 0.1792 test loss: 2.1023\n",
      "3m 26s (- 111m 11s) ([3/100] 3%), train loss: 0.1012 test loss: 0.0906  **\n",
      "4m 30s (- 108m 16s) ([4/100] 4%), train loss: 0.0823 test loss: 0.0707  **\n",
      "5m 36s (- 106m 23s) ([5/100] 5%), train loss: 0.0743 test loss: 0.0574  **\n",
      "6m 40s (- 104m 41s) ([6/100] 6%), train loss: 0.0746 test loss: 0.1273\n",
      "7m 46s (- 103m 17s) ([7/100] 7%), train loss: 0.0649 test loss: 0.0692\n",
      "8m 51s (- 101m 47s) ([8/100] 8%), train loss: 0.0634 test loss: 0.1025\n",
      "9m 55s (- 100m 17s) ([9/100] 9%), train loss: 0.0635 test loss: 0.0672\n",
      "10m 59s (- 98m 58s) ([10/100] 10%), train loss: 0.0612 test loss: 0.0754\n",
      "12m 4s (- 97m 38s) ([11/100] 11%), train loss: 0.0677 test loss: 0.0297  **\n",
      "13m 9s (- 96m 28s) ([12/100] 12%), train loss: 0.0778 test loss: 0.1092\n",
      "14m 13s (- 95m 8s) ([13/100] 13%), train loss: 0.0592 test loss: 0.1299\n"
     ]
    }
   ],
   "source": [
    "#%load_ext line_profiler\n",
    "%lprun -f trainnetwork trainnetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii , (X, label, img_orig) in enumerate(dataLoader[phase]):\n",
    "    print(ii)\n",
    "    X = X.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
